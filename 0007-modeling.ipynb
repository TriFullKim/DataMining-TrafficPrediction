{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import holidays\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Custome Module !!\n",
    "from module.dataframe import df_query_by_type, series_string_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kr_holidays = holidays.KR()\n",
    "\n",
    "tourism_road = [\"관광단지\", \"관광도로\"]\n",
    "general_road = [\"지방도\", \"일반국도\", \"길\", \"로\"]\n",
    "bridge_road = [\"교\", \"천교\"]\n",
    "nontaged_road = [\"-\"]\n",
    "\n",
    "\n",
    "def mapping_road(road_name):\n",
    "    if sum([target in road_name for target in tourism_road]) > 0:\n",
    "        return \"tourism_road\"\n",
    "    elif sum([target in road_name for target in general_road]) > 0:\n",
    "        return \"general_road\"\n",
    "    elif sum([target in road_name for target in bridge_road]) > 0:\n",
    "        return \"bridge\"\n",
    "    elif sum([target in road_name for target in nontaged_road]) > 0:\n",
    "        return \"-\"\n",
    "    else:\n",
    "        return \"-\"\n",
    "\n",
    "\n",
    "def to_date_str(int_date: int):\n",
    "    day = int_date % 100\n",
    "    year = (int_date - day) // 10000\n",
    "    month = (int_date - year * 10000) // 100\n",
    "    return f\"{year}-{month:02}-{day:02}\"\n",
    "\n",
    "\n",
    "# NOTE: 모든 값이 같은 열을 제거한다.\n",
    "def do_preprocessing(train, test):\n",
    "    # non-unique 제거하기\n",
    "    is_unique = train.apply(pd.Series.unique, axis=0).apply(len)\n",
    "    is_unique[is_unique == len(train)].index, is_unique[is_unique == 1].index\n",
    "    train = train.drop(columns=is_unique[is_unique == 1].index)\n",
    "    test = test.drop(columns=is_unique[is_unique == 1].index)\n",
    "\n",
    "    # weight_restricted - label추가하기\n",
    "    train[\"weight_restricted\"] = (train[\"weight_restricted\"] != 0).astype(int)\n",
    "    test[\"weight_restricted\"] = (test[\"weight_restricted\"] != 0).astype(int)\n",
    "\n",
    "    # is_holiday - label추가하기\n",
    "    train[\"is_holiday\"] = train[\"base_date\"].apply(to_date_str)\n",
    "    test[\"is_holiday\"] = test[\"base_date\"].apply(to_date_str)\n",
    "    train[\"is_holiday\"] = train[\"is_holiday\"].apply(lambda _X: _X in kr_holidays).astype(int)\n",
    "    test[\"is_holiday\"] = test[\"is_holiday\"].apply(lambda _X: _X in kr_holidays).astype(int)\n",
    "\n",
    "    # road_type - label추가하기\n",
    "    train[\"road_type\"] = train[\"road_name\"].apply(mapping_road)\n",
    "    test[\"road_type\"] = test[\"road_name\"].apply(mapping_road)\n",
    "\n",
    "    # label encode.\n",
    "    str_col = [\n",
    "        \"day_of_week\",\n",
    "        \"start_turn_restricted\",\n",
    "        \"end_turn_restricted\",\n",
    "        \"road_type\",\n",
    "        \"is_holiday\",\n",
    "        \"weight_restricted\",\n",
    "    ]\n",
    "    for col in str_col:\n",
    "        label_enc = LabelEncoder()\n",
    "        label_enc = label_enc.fit(train[col])\n",
    "        train[col] = label_enc.transform(train[col])\n",
    "\n",
    "        for label in np.unique(test[col]):\n",
    "            if label not in label_enc.classes_:\n",
    "                label_enc.classes_ = np.append(label_enc.classes_, label)\n",
    "        test[col] = label_enc.transform(test[col])\n",
    "\n",
    "    # One-Hot Enc\n",
    "    train = pd.get_dummies(\n",
    "        train,\n",
    "        columns=str_col,\n",
    "        dtype=int,\n",
    "    )\n",
    "    test = pd.get_dummies(\n",
    "        test,\n",
    "        columns=str_col,\n",
    "        dtype=int,\n",
    "    )\n",
    "\n",
    "    # 날짜 삽입입\n",
    "    train[\"base_date_year\"] = train[\"base_date\"] // 10000\n",
    "    train[\"base_date_month\"] = train[\"base_date\"] // 100 % 100\n",
    "    train[\"base_date_day\"] = train[\"base_date\"] % 100\n",
    "    train = train.drop(columns=\"base_date\")\n",
    "\n",
    "    test[\"base_date_year\"] = test[\"base_date\"] // 10000\n",
    "    test[\"base_date_month\"] = test[\"base_date\"] // 100 % 100\n",
    "    test[\"base_date_day\"] = test[\"base_date\"] % 100\n",
    "    test = test.drop(columns=\"base_date\")\n",
    "\n",
    "    # 문자형 데이터 제거하기.\n",
    "    train = pd.concat([df_query_by_type(train, float), df_query_by_type(train, int)], axis=1)\n",
    "    test = pd.concat([df_query_by_type(test, float), df_query_by_type(test, int)], axis=1)\n",
    "\n",
    "    # 좌표 정보 제거하기.\n",
    "    train = train.drop(columns=[\"start_latitude\", \"start_longitude\", \"end_latitude\", \"end_longitude\"])\n",
    "    test = test.drop(columns=[\"start_latitude\", \"start_longitude\", \"end_latitude\", \"end_longitude\"])\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet(\"./database/train.parquet\")\n",
    "test = pd.read_parquet(\"./database/test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = do_preprocessing(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = train.drop(columns=\"target\"), train[\"target\"]\n",
    "test_x = test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.072980 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 127\n",
      "[LightGBM] [Info] Number of data points in the train set: 3290851, number of used features: 28\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] Start training from score 42.779781\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058638 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 127\n",
      "[LightGBM] [Info] Number of data points in the train set: 3290851, number of used features: 28\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] Start training from score 42.788018\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066238 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 127\n",
      "[LightGBM] [Info] Number of data points in the train set: 3290851, number of used features: 28\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] Start training from score 42.787554\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060739 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 127\n",
      "[LightGBM] [Info] Number of data points in the train set: 3290851, number of used features: 28\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] Start training from score 42.794574\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066073 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 127\n",
      "[LightGBM] [Info] Number of data points in the train set: 3290851, number of used features: 28\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] Start training from score 42.796384\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062002 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 127\n",
      "[LightGBM] [Info] Number of data points in the train set: 3290851, number of used features: 28\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] Start training from score 42.781215\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062388 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 127\n",
      "[LightGBM] [Info] Number of data points in the train set: 3290851, number of used features: 28\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] Start training from score 42.789803\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059282 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 127\n",
      "[LightGBM] [Info] Number of data points in the train set: 3290851, number of used features: 28\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] Start training from score 42.785396\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.075733 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 127\n",
      "[LightGBM] [Info] Number of data points in the train set: 3290851, number of used features: 28\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] Start training from score 42.789504\n"
     ]
    }
   ],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(train_x, train_y, test_size=0.01, random_state=42)\n",
    "\n",
    "models = []\n",
    "for i in tqdm(range(10)):\n",
    "    x_train_part, x_eval, y_train_part, y_eval = train_test_split(train_x, train_y, test_size=0.3)\n",
    "    lgb_params = {\n",
    "        \"objective\": \"regression\",\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"metric\": \"rmse\",\n",
    "        \"n_estimators\": 750,\n",
    "        \"learning_rate\": 0.0882,\n",
    "        \"feature_fraction\": 0.8,\n",
    "        \"reg_lambda\": 0.001,\n",
    "        \"reg_alpha\": 0.0001,\n",
    "        \"subsample\": 0.85,\n",
    "        \"verbose\": 1,\n",
    "        \"num_leaves\": 190,\n",
    "    }\n",
    "    lgb_regress = LGBMRegressor(\n",
    "        **lgb_params,\n",
    "    )\n",
    "    eval_set = [(x_valid, y_valid), (x_eval, y_eval)]\n",
    "    lgb_regress.fit(x_train_part, y_train_part, eval_metric=\"rmse\", eval_set=eval_set)\n",
    "    models.append(lgb_regress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgb_params = {\n",
    "#     \"objective\": \"regression\",\n",
    "#     \"boosting_type\": \"gbdt\",\n",
    "#     \"metric\": \"rmse\",\n",
    "#     \"n_estimators\": 750,\n",
    "#     \"learning_rate\": 0.0882,\n",
    "#     \"feature_fraction\": 0.95,\n",
    "#     \"reg_lambda\": 0.001,\n",
    "#     \"reg_alpha\": 0.0001,\n",
    "#     \"subsample\": 0.85,\n",
    "#     \"verbose\": 2,\n",
    "#     'num_leaves': 190\n",
    "# }\n",
    "# lgb_regress = LGBMRegressor(\n",
    "#     **lgb_params,\n",
    "# )\n",
    "\n",
    "# lgb_regress.fit(x_train, y_train, eval_metric=\"rmse\", eval_set=eval_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5.672243434386836, 0.4447738314754437]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Eval\n",
    "# from sklearn.metrics import root_mean_squared_error as RMSE\n",
    "# from sklearn.metrics import median_absolute_error as MAE\n",
    "# from sklearn.metrics import r2_score as R2\n",
    "\n",
    "# [\n",
    "#     MAE(lgb_regress.predict(x_valid), y_valid),\n",
    "#     R2(lgb_regress.predict(x_valid), y_valid),\n",
    "# ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('./database/sample_submission.csv')\n",
    "final_submission = pd.read_csv('./database/sample_submission.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgb_params = {\n",
    "#     \"objective\": \"regression\",\n",
    "#     \"boosting_type\": \"gbdt\",\n",
    "#     \"metric\": \"rmse\",\n",
    "#     \"n_estimators\": 1500,\n",
    "#     \"learning_rate\": 0.09,\n",
    "#     \"feature_fraction\": 0.95,\n",
    "#     \"reg_lambda\": 0.001,\n",
    "#     \"reg_alpha\": 0.0001,\n",
    "#     \"subsample\": 0.85,\n",
    "#     \"verbose\": 2,\n",
    "#     'num_leaves': 90\n",
    "# }\n",
    "# [5.746946134178014, 0.43311401006703243]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgb_params = {\n",
    "#     \"objective\": \"regression\",\n",
    "#     \"boosting_type\": \"gbdt\",\n",
    "#     \"metric\": \"rmse\",\n",
    "#     \"n_estimators\": 1500,\n",
    "#     \"learning_rate\": 0.0885,\n",
    "#     \"feature_fraction\": 0.95,\n",
    "#     \"reg_lambda\": 0.001,\n",
    "#     \"reg_alpha\": 0.0001,\n",
    "#     \"subsample\": 0.85,\n",
    "#     \"verbose\": 2,\n",
    "#     'num_leaves': 90\n",
    "# }\n",
    "# [5.747548193187486, 0.43289453456149374]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE:1\n",
    "# [5.761627551595902, 0.43105196246630095]\n",
    "# -> 'num_leaves': 256\n",
    "# [5.751544901162482, 0.4324136364045469]\n",
    "# -> 'num_leaves': 190\n",
    "# [5.752340975253492, 0.4324743850641275]\n",
    "# -> 'num_leaves': 180\n",
    "# lgb_params = {\n",
    "#     \"objective\": \"regression\",\n",
    "#     \"boosting_type\": \"gbdt\",\n",
    "#     \"metric\": \"rmse\",\n",
    "#     \"n_estimators\": 1500,\n",
    "#     \"learning_rate\": 0.0882,\n",
    "#     \"feature_fraction\": 0.95,\n",
    "#     \"reg_lambda\": 0.001,\n",
    "#     \"reg_alpha\": 0.0001,\n",
    "#     \"subsample\": 0.85,\n",
    "#     \"verbose\": 2,\n",
    "#     'num_leaves': 90\n",
    "# }\n",
    "# [5.79309681117978, 0.42550610451006965]\n",
    "# -> 'num_leaves': 48\n",
    "# [5.7543164050935545, 0.4317961056989281]\n",
    "# -> 'num_leaves': 10\n",
    "# [5.838684291356461, 0.4139329591331222]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE:2\n",
    "# lgb_params = {\n",
    "#     \"objective\": \"regression\",\n",
    "#     \"boosting_type\": \"gbdt\",\n",
    "#     \"metric\": \"rmse\",\n",
    "#     \"n_estimators\": 2000,\n",
    "#     \"learning_rate\": 0.0882,\n",
    "#     \"feature_fraction\": 0.95,\n",
    "#     \"reg_lambda\": 0.001,\n",
    "#     \"reg_alpha\": 0.0001,\n",
    "#     \"subsample\": 0.85,\n",
    "#     \"verbose\": 2,\n",
    "#     'num_leaves': 190\n",
    "# }\n",
    "# [5.757305149277347, 0.43135064392503075]\n",
    "# n_estimators -> 1500\n",
    "# [5.751544901162482, 0.4324136364045469]\n",
    "# n_estimators -> 900\n",
    "# [5.748957234746737, 0.432866128090696]\n",
    "# n_estimators -> 850\n",
    "# [5.748207896096723, 0.4329333047085594]\n",
    "# n_estimators -> 800\n",
    "# [5.7471590588239, 0.4329834431038184]\n",
    "# n_estimators -> 750\n",
    "# [5.74673096535663, 0.4329565497995935]\n",
    "# n_estimators -> 700\n",
    "# [5.748021316320747, 0.43296032209695046]\n",
    "# n_estimators -> 600\n",
    "# [5.74997947849212, 0.4327542009939448]\n",
    "# n_estimators -> 400\n",
    "# [5.753948085151837, 0.43145049845013994]\n",
    "# n_estimators -> 200\n",
    "# [5.769750016732333, 0.42777351387225127]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE:3\n",
    "# -> 1\n",
    "# [5.747376345440024, 0.43301548771802834]\n",
    "# lgb_params = { ############## 이거씀씀\n",
    "#     \"objective\": \"regression\",\n",
    "#     \"boosting_type\": \"gbdt\",\n",
    "#     \"metric\": \"rmse\",\n",
    "#     \"n_estimators\": 750,\n",
    "#     \"learning_rate\": 0.0882,\n",
    "#     \"feature_fraction\": 0.95,\n",
    "#     \"reg_lambda\": 0.001,\n",
    "#     \"reg_alpha\": 0.0001,\n",
    "#     \"subsample\": 0.85,\n",
    "#     \"verbose\": 2,\n",
    "#     'num_leaves': 190\n",
    "# }\n",
    "# [5.74673096535663, 0.4329565497995935]\n",
    "# feature_fraction -> 0.7\n",
    "# [5.751887897278465, 0.4317971745459652]\n",
    "# feature_fraction -> 0.3\n",
    "# [5.902017632629661, 0.4022951157263006]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgb_params = {\n",
    "#     \"objective\": \"regression\",\n",
    "#     \"boosting_type\": \"gbdt\",\n",
    "#     \"metric\": \"rmse\",\n",
    "#     \"n_estimators\": 1500,\n",
    "#     \"learning_rate\": 0.088,\n",
    "#     \"feature_fraction\": 0.95,\n",
    "#     \"reg_lambda\": 0.001,\n",
    "#     \"reg_alpha\": 0.0001,\n",
    "#     \"subsample\": 0.85,\n",
    "#     \"verbose\": 2,\n",
    "#     'num_leaves': 90\n",
    "# }\n",
    "# [5.745848408718423, 0.43343802094811135]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgb_params = {\n",
    "#     \"objective\": \"regression\",\n",
    "#     \"boosting_type\": \"gbdt\",\n",
    "#     \"metric\": \"rmse\",\n",
    "#     \"n_estimators\": 1500,\n",
    "#     \"learning_rate\": 0.085,\n",
    "#     \"feature_fraction\": 0.95,\n",
    "#     \"reg_lambda\": 0.001,\n",
    "#     \"reg_alpha\": 0.0001,\n",
    "#     \"subsample\": 0.85,\n",
    "#     \"verbose\": 2,\n",
    "#     'num_leaves': 90\n",
    "# }\n",
    "# [5.746014257796521, 0.4331837299248704]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lgb_params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"metric\": \"rmse\",\n",
    "    \"n_estimators\": 1500,\n",
    "    \"learning_rate\": 0.08,\n",
    "    \"feature_fraction\": 0.95,\n",
    "    \"reg_lambda\": 0.001,\n",
    "    \"reg_alpha\": 0.0001,\n",
    "    \"subsample\": 0.85,\n",
    "    \"verbose\": 2,\n",
    "    'num_leaves': 90\n",
    "}\n",
    "[5.746019162681328, 0.43304242874413157]\n",
    "\n",
    "import pickle\n",
    "with open(\"./model/lgb_regress-ens-6.pkl\",\"wb\") as f:\n",
    "    pickle.dump(models,f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n"
     ]
    }
   ],
   "source": [
    "sample_submission[\"target\"] = models[0].predict(test_x)\n",
    "for idx,lgb_model in enumerate(models[1:]):\n",
    "    sample_submission[idx] = lgb_model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_000000</td>\n",
       "      <td>23.372196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_000001</td>\n",
       "      <td>47.294446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_000002</td>\n",
       "      <td>47.929473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_000003</td>\n",
       "      <td>37.162168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_000004</td>\n",
       "      <td>39.868535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291236</th>\n",
       "      <td>TEST_291236</td>\n",
       "      <td>48.519542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291237</th>\n",
       "      <td>TEST_291237</td>\n",
       "      <td>37.438202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291238</th>\n",
       "      <td>TEST_291238</td>\n",
       "      <td>22.898877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291239</th>\n",
       "      <td>TEST_291239</td>\n",
       "      <td>26.570180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291240</th>\n",
       "      <td>TEST_291240</td>\n",
       "      <td>41.628529</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>291241 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id     target\n",
       "0       TEST_000000  23.372196\n",
       "1       TEST_000001  47.294446\n",
       "2       TEST_000002  47.929473\n",
       "3       TEST_000003  37.162168\n",
       "4       TEST_000004  39.868535\n",
       "...             ...        ...\n",
       "291236  TEST_291236  48.519542\n",
       "291237  TEST_291237  37.438202\n",
       "291238  TEST_291238  22.898877\n",
       "291239  TEST_291239  26.570180\n",
       "291240  TEST_291240  41.628529\n",
       "\n",
       "[291241 rows x 2 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_submission[\"target\"]=sample_submission.drop(columns=\"id\").mean(axis=1)\n",
    "final_submission.to_csv(\"./lgb_regress-ens-6.csv\", index = False)\n",
    "final_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgb_params = {\n",
    "#     \"objective\": \"regression\",\n",
    "#     \"boosting_type\": \"gbdt\",\n",
    "#     \"metric\": \"rmse\",\n",
    "#     \"n_estimators\": 1500,\n",
    "#     \"learning_rate\": 0.07,\n",
    "#     \"feature_fraction\": 0.95,\n",
    "#     \"reg_lambda\": 0.001,\n",
    "#     \"reg_alpha\": 0.0001,\n",
    "#     \"subsample\": 0.85,\n",
    "#     \"verbose\": 2,\n",
    "#     'num_leaves': 90\n",
    "# }\n",
    "# [5.747496090135723, 0.4327742262599782]\n",
    "# 6.095245684\n",
    "# import pickle\n",
    "# with open(\"./model/lgb_regress-3.pkl\",\"wb\") as f:\n",
    "#     pickle.dump(lgb_regress,f)\n",
    "# sample_submission[\"target\"] = lgb_regress.predict(test_x)\n",
    "# sample_submission.to_csv(\"./lgb_regress-3.csv\", index = False)\n",
    "# sample_submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![decisiontree](./0004.png)\n",
    "![linear](./0004.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "damine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
